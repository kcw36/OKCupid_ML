{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To download the profile dataset use attached link <br />\n",
    "https://content.codecademy.com/PRO/paths/data-science/OKCupid-Date-A-Scientist-Starter.zip?_gl=1*1fcshk0*_ga*NzEyNTMwNzI0Ny4xNjU2MzMyNzI3*_ga_3LRZM6TM9L*MTY2MjM5OTY1MS4xMTAuMC4xNjYyMzk5NjUxLjYwLjAuMA.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules for data exploration\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and explore data for understanding\n",
    "df = pd.read_csv(r'C:\\Users\\kylew\\Codecademy\\profiles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59946 entries, 0 to 59945\n",
      "Data columns (total 31 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   age          59946 non-null  int64  \n",
      " 1   body_type    54650 non-null  object \n",
      " 2   diet         35551 non-null  object \n",
      " 3   drinks       56961 non-null  object \n",
      " 4   drugs        45866 non-null  object \n",
      " 5   education    53318 non-null  object \n",
      " 6   essay0       54458 non-null  object \n",
      " 7   essay1       52374 non-null  object \n",
      " 8   essay2       50308 non-null  object \n",
      " 9   essay3       48470 non-null  object \n",
      " 10  essay4       49409 non-null  object \n",
      " 11  essay5       49096 non-null  object \n",
      " 12  essay6       46175 non-null  object \n",
      " 13  essay7       47495 non-null  object \n",
      " 14  essay8       40721 non-null  object \n",
      " 15  essay9       47343 non-null  object \n",
      " 16  ethnicity    54266 non-null  object \n",
      " 17  height       59943 non-null  float64\n",
      " 18  income       59946 non-null  int64  \n",
      " 19  job          51748 non-null  object \n",
      " 20  last_online  59946 non-null  object \n",
      " 21  location     59946 non-null  object \n",
      " 22  offspring    24385 non-null  object \n",
      " 23  orientation  59946 non-null  object \n",
      " 24  pets         40025 non-null  object \n",
      " 25  religion     39720 non-null  object \n",
      " 26  sex          59946 non-null  object \n",
      " 27  sign         48890 non-null  object \n",
      " 28  smokes       54434 non-null  object \n",
      " 29  speaks       59896 non-null  object \n",
      " 30  status       59946 non-null  object \n",
      "dtypes: float64(1), int64(2), object(28)\n",
      "memory usage: 14.2+ MB\n",
      "None\n",
      "                age        height          income\n",
      "count  59946.000000  59943.000000    59946.000000\n",
      "mean      32.340290     68.295281    20033.222534\n",
      "std        9.452779      3.994803    97346.192104\n",
      "min       18.000000      1.000000       -1.000000\n",
      "25%       26.000000     66.000000       -1.000000\n",
      "50%       30.000000     68.000000       -1.000000\n",
      "75%       37.000000     71.000000       -1.000000\n",
      "max      110.000000     95.000000  1000000.000000\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a little extra' 'average' 'thin' 'athletic' 'fit' nan 'skinny' 'curvy'\n",
      " 'full figured' 'jacked' 'rather not say' 'used up' 'overweight']\n",
      "['strictly anything' 'mostly other' 'anything' 'vegetarian' nan\n",
      " 'mostly anything' 'mostly vegetarian' 'strictly vegan'\n",
      " 'strictly vegetarian' 'mostly vegan' 'strictly other' 'mostly halal'\n",
      " 'other' 'vegan' 'mostly kosher' 'strictly halal' 'halal'\n",
      " 'strictly kosher' 'kosher']\n",
      "['socially' 'often' 'not at all' 'rarely' nan 'very often' 'desperately']\n",
      "['never' 'sometimes' nan 'often']\n",
      "['working on college/university' 'working on space camp'\n",
      " 'graduated from masters program' 'graduated from college/university'\n",
      " 'working on two-year college' nan 'graduated from high school'\n",
      " 'working on masters program' 'graduated from space camp'\n",
      " 'college/university' 'dropped out of space camp'\n",
      " 'graduated from ph.d program' 'graduated from law school'\n",
      " 'working on ph.d program' 'two-year college'\n",
      " 'graduated from two-year college' 'working on med school'\n",
      " 'dropped out of college/university' 'space camp'\n",
      " 'graduated from med school' 'dropped out of high school'\n",
      " 'working on high school' 'masters program' 'dropped out of ph.d program'\n",
      " 'dropped out of two-year college' 'dropped out of med school'\n",
      " 'high school' 'working on law school' 'law school'\n",
      " 'dropped out of masters program' 'ph.d program'\n",
      " 'dropped out of law school' 'med school']\n",
      "['asian, white' 'white' nan 'asian, black, other' 'white, other'\n",
      " 'hispanic / latin, white' 'hispanic / latin' 'pacific islander, white'\n",
      " 'asian' 'black, white' 'pacific islander' 'asian, native american'\n",
      " 'asian, pacific islander' 'black, native american, white'\n",
      " 'middle eastern, other' 'native american, white' 'indian' 'black'\n",
      " 'black, native american, hispanic / latin, other'\n",
      " 'black, native american, hispanic / latin'\n",
      " 'asian, black, pacific islander'\n",
      " 'asian, middle eastern, black, native american, indian, pacific islander, hispanic / latin, white, other'\n",
      " 'other' 'hispanic / latin, other' 'asian, black' 'middle eastern, white'\n",
      " 'native american, white, other' 'black, native american'\n",
      " 'black, white, other' 'hispanic / latin, white, other' 'middle eastern'\n",
      " 'black, other' 'native american, hispanic / latin, white' 'black, indian'\n",
      " 'indian, white, other' 'middle eastern, indian, other'\n",
      " 'black, native american, hispanic / latin, white, other'\n",
      " 'pacific islander, hispanic / latin' 'black, hispanic / latin, white'\n",
      " 'native american' 'indian, white' 'asian, white, other'\n",
      " 'black, hispanic / latin' 'asian, hispanic / latin, white'\n",
      " 'middle eastern, hispanic / latin'\n",
      " 'asian, black, native american, pacific islander, white'\n",
      " 'middle eastern, indian' 'asian, indian' 'pacific islander, other'\n",
      " 'black, native american, white, other' 'black, pacific islander'\n",
      " 'middle eastern, native american, white'\n",
      " 'asian, native american, white, other'\n",
      " 'pacific islander, hispanic / latin, white' 'indian, other'\n",
      " 'asian, pacific islander, other' 'black, hispanic / latin, other'\n",
      " 'asian, black, native american'\n",
      " 'black, native american, hispanic / latin, white'\n",
      " 'native american, hispanic / latin' 'indian, hispanic / latin'\n",
      " 'native american, pacific islander'\n",
      " 'asian, black, native american, hispanic / latin, white'\n",
      " 'asian, black, white'\n",
      " 'asian, black, native american, pacific islander, other'\n",
      " 'middle eastern, hispanic / latin, white'\n",
      " 'asian, pacific islander, white'\n",
      " 'asian, native american, hispanic / latin, white, other'\n",
      " 'asian, hispanic / latin' 'asian, pacific islander, white, other'\n",
      " 'middle eastern, white, other'\n",
      " 'asian, pacific islander, hispanic / latin'\n",
      " 'black, native american, indian, other'\n",
      " 'native american, hispanic / latin, white, other'\n",
      " 'black, native american, other' 'asian, other'\n",
      " 'middle eastern, hispanic / latin, other'\n",
      " 'pacific islander, hispanic / latin, white, other'\n",
      " 'asian, black, hispanic / latin'\n",
      " 'asian, pacific islander, hispanic / latin, white'\n",
      " 'asian, black, native american, white'\n",
      " 'asian, middle eastern, white, other'\n",
      " 'native american, pacific islander, hispanic / latin'\n",
      " 'asian, native american, white'\n",
      " 'native american, pacific islander, hispanic / latin, white, other'\n",
      " 'indian, pacific islander' 'asian, middle eastern, black'\n",
      " 'asian, middle eastern, indian' 'asian, middle eastern, white'\n",
      " 'pacific islander, white, other'\n",
      " 'black, pacific islander, hispanic / latin' 'asian, middle eastern'\n",
      " 'asian, hispanic / latin, other'\n",
      " 'middle eastern, black, native american, indian, white, other'\n",
      " 'middle eastern, pacific islander, other' 'middle eastern, black'\n",
      " 'asian, indian, pacific islander'\n",
      " 'black, native american, pacific islander' 'native american, indian'\n",
      " 'asian, middle eastern, black, native american, indian, pacific islander, hispanic / latin, white'\n",
      " 'black, indian, other'\n",
      " 'asian, middle eastern, indian, hispanic / latin, white, other'\n",
      " 'middle eastern, black, white' 'asian, hispanic / latin, white, other'\n",
      " 'native american, hispanic / latin, other'\n",
      " 'middle eastern, black, pacific islander, white'\n",
      " 'asian, black, native american, hispanic / latin'\n",
      " 'native american, other' 'black, indian, white'\n",
      " 'asian, native american, hispanic / latin, white'\n",
      " 'black, native american, indian, white'\n",
      " 'middle eastern, black, indian, pacific islander, hispanic / latin, white'\n",
      " 'middle eastern, hispanic / latin, white, other'\n",
      " 'asian, black, native american, other'\n",
      " 'native american, pacific islander, hispanic / latin, white'\n",
      " 'asian, indian, other'\n",
      " 'middle eastern, native american, hispanic / latin, white, other'\n",
      " 'asian, middle eastern, black, pacific islander, hispanic / latin, white'\n",
      " 'black, native american, pacific islander, hispanic / latin, white, other'\n",
      " 'asian, middle eastern, native american, hispanic / latin, white'\n",
      " 'asian, middle eastern, black, native american, pacific islander, hispanic / latin, white, other'\n",
      " 'asian, indian, white' 'native american, pacific islander, white, other'\n",
      " 'middle eastern, black, native american, indian, pacific islander, hispanic / latin, white'\n",
      " 'asian, middle eastern, other' 'middle eastern, pacific islander'\n",
      " 'asian, black, hispanic / latin, other'\n",
      " 'asian, middle eastern, black, native american, hispanic / latin, white'\n",
      " 'middle eastern, black, hispanic / latin'\n",
      " 'black, pacific islander, white'\n",
      " 'asian, middle eastern, black, native american, indian, pacific islander, hispanic / latin, other'\n",
      " 'middle eastern, black, native american, indian, hispanic / latin, white'\n",
      " 'asian, pacific islander, hispanic / latin, white, other'\n",
      " 'middle eastern, indian, white' 'asian, indian, white, other'\n",
      " 'middle eastern, black, native american, white, other'\n",
      " 'black, native american, pacific islander, other'\n",
      " 'middle eastern, black, native american, white'\n",
      " 'asian, indian, pacific islander, other'\n",
      " 'asian, black, native american, white, other'\n",
      " 'black, indian, hispanic / latin, white'\n",
      " 'asian, middle eastern, black, native american, indian, pacific islander, white'\n",
      " 'asian, black, pacific islander, hispanic / latin'\n",
      " 'middle eastern, black, native american, indian, pacific islander, hispanic / latin, white, other'\n",
      " 'asian, black, native american, indian'\n",
      " 'asian, black, indian, hispanic / latin, other'\n",
      " 'indian, hispanic / latin, other' 'asian, indian, hispanic / latin'\n",
      " 'asian, native american, pacific islander, white, other'\n",
      " 'asian, black, native american, indian, hispanic / latin, white, other'\n",
      " 'asian, indian, hispanic / latin, white'\n",
      " 'pacific islander, hispanic / latin, other'\n",
      " 'asian, indian, pacific islander, hispanic / latin, white, other'\n",
      " 'indian, hispanic / latin, white'\n",
      " 'asian, native american, pacific islander, hispanic / latin, white, other'\n",
      " 'asian, pacific islander, hispanic / latin, other'\n",
      " 'asian, black, hispanic / latin, white, other'\n",
      " 'black, indian, hispanic / latin'\n",
      " 'middle eastern, black, native american, hispanic / latin, white'\n",
      " 'black, pacific islander, other'\n",
      " 'black, native american, pacific islander, white'\n",
      " 'asian, black, native american, pacific islander'\n",
      " 'asian, indian, hispanic / latin, other'\n",
      " 'middle eastern, native american'\n",
      " 'middle eastern, native american, hispanic / latin'\n",
      " 'black, hispanic / latin, white, other'\n",
      " 'asian, native american, pacific islander, hispanic / latin, white'\n",
      " 'asian, native american, hispanic / latin'\n",
      " 'black, native american, indian, hispanic / latin, white, other'\n",
      " 'asian, middle eastern, hispanic / latin, white'\n",
      " 'black, native american, pacific islander, white, other'\n",
      " 'native american, indian, pacific islander, hispanic / latin'\n",
      " 'black, indian, white, other'\n",
      " 'asian, middle eastern, native american, pacific islander, hispanic / latin, white, other'\n",
      " 'native american, pacific islander, white'\n",
      " 'middle eastern, indian, white, other' 'asian, black, white, other'\n",
      " 'middle eastern, native american, hispanic / latin, white'\n",
      " 'indian, hispanic / latin, white, other'\n",
      " 'asian, middle eastern, black, pacific islander'\n",
      " 'asian, middle eastern, black, indian, pacific islander, hispanic / latin, white'\n",
      " 'asian, middle eastern, indian, other'\n",
      " 'asian, middle eastern, black, white, other'\n",
      " 'black, native american, pacific islander, hispanic / latin, white'\n",
      " 'black, native american, indian, pacific islander, hispanic / latin'\n",
      " 'asian, black, pacific islander, white'\n",
      " 'middle eastern, pacific islander, hispanic / latin'\n",
      " 'black, native american, indian, white, other'\n",
      " 'asian, black, hispanic / latin, white'\n",
      " 'asian, black, native american, indian, pacific islander, white'\n",
      " 'asian, black, native american, indian, pacific islander, hispanic / latin'\n",
      " 'asian, middle eastern, hispanic / latin, white, other'\n",
      " 'middle eastern, black, native american, indian'\n",
      " 'asian, native american, pacific islander'\n",
      " 'asian, black, native american, pacific islander, white, other'\n",
      " 'asian, middle eastern, hispanic / latin'\n",
      " 'asian, black, pacific islander, other'\n",
      " 'asian, native american, indian, pacific islander, hispanic / latin, white'\n",
      " 'middle eastern, native american, white, other'\n",
      " 'asian, native american, hispanic / latin, other'\n",
      " 'native american, indian, white'\n",
      " 'black, native american, pacific islander, hispanic / latin'\n",
      " 'asian, native american, pacific islander, white'\n",
      " 'black, native american, indian'\n",
      " 'indian, pacific islander, hispanic / latin, white'\n",
      " 'asian, middle eastern, black, native american, indian, pacific islander, hispanic / latin'\n",
      " 'asian, middle eastern, indian, hispanic / latin'\n",
      " 'asian, middle eastern, native american, pacific islander, other'\n",
      " 'black, native american, indian, pacific islander'\n",
      " 'asian, middle eastern, native american, pacific islander, white, other'\n",
      " 'asian, native american, other' 'middle eastern, black, other'\n",
      " 'asian, black, pacific islander, hispanic / latin, white'\n",
      " 'asian, middle eastern, native american, indian, pacific islander, hispanic / latin, white'\n",
      " 'asian, native american, indian, pacific islander, hispanic / latin, white, other'\n",
      " 'asian, middle eastern, black, pacific islander, hispanic / latin'\n",
      " 'asian, black, pacific islander, white, other' 'asian, black, indian']\n",
      "['transportation' 'hospitality / travel' nan 'student'\n",
      " 'artistic / musical / writer' 'computer / hardware / software'\n",
      " 'banking / financial / real estate' 'entertainment / media'\n",
      " 'sales / marketing / biz dev' 'other' 'medicine / health'\n",
      " 'science / tech / engineering' 'executive / management'\n",
      " 'education / academia' 'clerical / administrative'\n",
      " 'construction / craftsmanship' 'rather not say' 'political / government'\n",
      " 'law / legal services' 'unemployed' 'military' 'retired']\n",
      "['2012-06-28-20-30' '2012-06-29-21-41' '2012-06-27-09-10' ...\n",
      " '2012-06-02-08-16' '2012-02-17-20-44' '2012-06-14-16-51']\n",
      "['south san francisco, california' 'oakland, california'\n",
      " 'san francisco, california' 'berkeley, california'\n",
      " 'belvedere tiburon, california' 'san mateo, california'\n",
      " 'daly city, california' 'san leandro, california' 'atherton, california'\n",
      " 'san rafael, california' 'walnut creek, california'\n",
      " 'menlo park, california' 'belmont, california' 'san jose, california'\n",
      " 'palo alto, california' 'emeryville, california' 'el granada, california'\n",
      " 'castro valley, california' 'fairfax, california'\n",
      " 'mountain view, california' 'burlingame, california'\n",
      " 'martinez, california' 'pleasant hill, california' 'hayward, california'\n",
      " 'alameda, california' 'vallejo, california' 'benicia, california'\n",
      " 'el cerrito, california' 'mill valley, california' 'richmond, california'\n",
      " 'redwood city, california' 'el sobrante, california'\n",
      " 'stanford, california' 'san pablo, california' 'novato, california'\n",
      " 'pacifica, california' 'lafayette, california'\n",
      " 'half moon bay, california' 'fremont, california' 'orinda, california'\n",
      " 'san anselmo, california' 'corte madera, california' 'albany, california'\n",
      " 'san carlos, california' 'san lorenzo, california'\n",
      " 'foster city, california' 'hercules, california' 'santa cruz, california'\n",
      " 'bolinas, california' 'sausalito, california' 'millbrae, california'\n",
      " 'larkspur, california' 'moraga, california' 'san bruno, california'\n",
      " 'petaluma, california' 'pinole, california' 'san geronimo, california'\n",
      " 'crockett, california' 'boulder, colorado' 'brisbane, california'\n",
      " 'freedom, california' 'montara, california' 'green brae, california'\n",
      " 'woodside, california' 'new york, new york' 'ross, california'\n",
      " 'east palo alto, california' 'san quentin, california' 'portland, oregon'\n",
      " 'rodeo, california' 'hacienda heights, california' 'woodacre, california'\n",
      " 'westlake, california' 'riverside, california' 'rohnert park, california'\n",
      " 'sacramento, california' 'point richmond, california'\n",
      " 'san diego, california' 'canyon country, california' 'tucson, arizona'\n",
      " 'honolulu, hawaii' 'billings, montana' 'west oakland, california'\n",
      " 'kentfield, california' 'milwaukee, wisconsin' 'woodbridge, virginia'\n",
      " 'glencove, california' 'tiburon, california' 'madrid, spain'\n",
      " 'las vegas, nevada' 'peoria, illinois' 'santa monica, california'\n",
      " 'bellwood, illinois' 'los angeles, california' 'moss beach, california'\n",
      " 'nha trang, vietnam' 'hillsborough, california' 'olema, california'\n",
      " 'union city, california' 'colma, california' 'cork, ireland'\n",
      " 'new orleans, louisiana' 'kensington, california'\n",
      " 'redwood shores, california' 'utica, michigan' 'brea, california'\n",
      " 'lagunitas, california' 'stinson beach, california'\n",
      " 'santa clara, california' 'studio city, california' 'concord, california'\n",
      " 'piedmont, california' 'grand rapids, michigan' 'seaside, california'\n",
      " 'leander, texas' 'forest knolls, california' 'edinburgh, united kingdom'\n",
      " 'magalia, california' 'london, united kingdom' 'astoria, new york'\n",
      " 'chicago, illinois' 'orange, california' 'south wellfleet, massachusetts'\n",
      " 'bayshore, california' 'asheville, north carolina'\n",
      " 'los gatos, california' 'boise, idaho' 'islip terrace, new york'\n",
      " 'sunnyvale, california' 'cambridge, massachusetts' 'lake orion, michigan'\n",
      " 'ozone park, new york' 'jackson, mississippi' 'ashland, california'\n",
      " 'south orange, new jersey' 'fort lauderdale, florida'\n",
      " 'minneapolis, minnesota' 'pasadena, california' 'atlanta, georgia'\n",
      " 'salt lake city, utah' 'arcadia, california' 'milpitas, california'\n",
      " 'san antonio, texas' 'port costa, california' 'nicasio, california'\n",
      " 'livingston, california' 'bellingham, washington' 'crowley, texas'\n",
      " 'boston, massachusetts' 'longwood, florida' 'fayetteville, west virginia'\n",
      " 'granite bay, california' 'isla vista, california' 'hilarita, california'\n",
      " 'campbell, california' 'stratford, connecticut' 'santa ana, california'\n",
      " 'santa rosa, california' 'kula, hawaii' 'murfreesboro, tennessee'\n",
      " 'brooklyn, new york' 'north hollywood, california'\n",
      " 'nevada city, california' 'providence, rhode island'\n",
      " 'stockton, california' 'marin city, california'\n",
      " 'washington, district of columbia' 'waterford, california'\n",
      " 'vancouver, british columbia, canada' 'muir beach, california'\n",
      " 'pacheco, california' 'irvine, california' 'kansas city, missouri'\n",
      " 'kassel, germany' 'canyon, california' 'philadelphia, pennsylvania'\n",
      " 'oceanview, california' 'long beach, new york' 'amsterdam, netherlands'\n",
      " 'taunton, massachusetts' 'napa, california' 'austin, texas'\n",
      " 'san luis obispo, california' 'modesto, california'\n",
      " 'bonaduz, switzerland' 'costa mesa, california' 'guadalajara, mexico'\n",
      " 'oakley, california' 'columbus, ohio' 'chico, california'\n",
      " 'south lake tahoe, california' 'vacaville, california' 'miami, florida'\n",
      " 'long beach, california' 'denver, colorado' 'seattle, washington'\n",
      " 'cincinnati, ohio' 'phoenix, arizona' 'rochester, michigan']\n",
      "['doesn&rsquo;t have kids, but might want them' nan\n",
      " 'doesn&rsquo;t want kids' 'doesn&rsquo;t have kids, but wants them'\n",
      " 'doesn&rsquo;t have kids' 'wants kids' 'has a kid' 'has kids'\n",
      " 'doesn&rsquo;t have kids, and doesn&rsquo;t want any'\n",
      " 'has kids, but doesn&rsquo;t want more'\n",
      " 'has a kid, but doesn&rsquo;t want more' 'has a kid, and wants more'\n",
      " 'has kids, and might want more' 'might want kids'\n",
      " 'has a kid, and might want more' 'has kids, and wants more']\n",
      "['straight' 'bisexual' 'gay']\n",
      "['likes dogs and likes cats' 'has cats' 'likes cats' nan\n",
      " 'has dogs and likes cats' 'likes dogs and has cats'\n",
      " 'likes dogs and dislikes cats' 'has dogs' 'has dogs and dislikes cats'\n",
      " 'likes dogs' 'has dogs and has cats' 'dislikes dogs and has cats'\n",
      " 'dislikes dogs and dislikes cats' 'dislikes cats'\n",
      " 'dislikes dogs and likes cats' 'dislikes dogs']\n",
      "['agnosticism and very serious about it'\n",
      " 'agnosticism but not too serious about it' nan 'atheism' 'christianity'\n",
      " 'christianity but not too serious about it'\n",
      " 'atheism and laughing about it' 'christianity and very serious about it'\n",
      " 'other' 'catholicism' 'catholicism but not too serious about it'\n",
      " 'catholicism and somewhat serious about it'\n",
      " 'agnosticism and somewhat serious about it'\n",
      " 'catholicism and laughing about it' 'agnosticism and laughing about it'\n",
      " 'agnosticism' 'atheism and somewhat serious about it'\n",
      " 'buddhism but not too serious about it'\n",
      " 'other but not too serious about it' 'buddhism'\n",
      " 'other and laughing about it' 'judaism but not too serious about it'\n",
      " 'buddhism and laughing about it' 'other and somewhat serious about it'\n",
      " 'other and very serious about it' 'hinduism but not too serious about it'\n",
      " 'atheism but not too serious about it' 'judaism'\n",
      " 'christianity and somewhat serious about it'\n",
      " 'hinduism and very serious about it' 'atheism and very serious about it'\n",
      " 'judaism and laughing about it' 'christianity and laughing about it'\n",
      " 'hinduism and laughing about it' 'buddhism and somewhat serious about it'\n",
      " 'islam and very serious about it' 'islam' 'hinduism'\n",
      " 'judaism and somewhat serious about it'\n",
      " 'catholicism and very serious about it'\n",
      " 'judaism and very serious about it'\n",
      " 'hinduism and somewhat serious about it'\n",
      " 'islam but not too serious about it' 'buddhism and very serious about it'\n",
      " 'islam and laughing about it' 'islam and somewhat serious about it']\n",
      "['m' 'f']\n",
      "['gemini' 'cancer' 'pisces but it doesn&rsquo;t matter' 'pisces'\n",
      " 'aquarius' 'taurus' 'virgo' 'sagittarius'\n",
      " 'gemini but it doesn&rsquo;t matter' 'cancer but it doesn&rsquo;t matter'\n",
      " 'leo but it doesn&rsquo;t matter' nan\n",
      " 'aquarius but it doesn&rsquo;t matter'\n",
      " 'aries and it&rsquo;s fun to think about'\n",
      " 'libra but it doesn&rsquo;t matter'\n",
      " 'pisces and it&rsquo;s fun to think about' 'libra'\n",
      " 'taurus but it doesn&rsquo;t matter'\n",
      " 'sagittarius but it doesn&rsquo;t matter' 'scorpio and it matters a lot'\n",
      " 'gemini and it&rsquo;s fun to think about'\n",
      " 'leo and it&rsquo;s fun to think about'\n",
      " 'cancer and it&rsquo;s fun to think about'\n",
      " 'libra and it&rsquo;s fun to think about'\n",
      " 'aquarius and it&rsquo;s fun to think about'\n",
      " 'virgo but it doesn&rsquo;t matter'\n",
      " 'scorpio and it&rsquo;s fun to think about'\n",
      " 'capricorn but it doesn&rsquo;t matter' 'scorpio'\n",
      " 'capricorn and it&rsquo;s fun to think about' 'leo'\n",
      " 'aries but it doesn&rsquo;t matter' 'aries'\n",
      " 'scorpio but it doesn&rsquo;t matter'\n",
      " 'sagittarius and it&rsquo;s fun to think about'\n",
      " 'libra and it matters a lot' 'taurus and it&rsquo;s fun to think about'\n",
      " 'leo and it matters a lot' 'virgo and it&rsquo;s fun to think about'\n",
      " 'cancer and it matters a lot' 'capricorn' 'pisces and it matters a lot'\n",
      " 'aries and it matters a lot' 'capricorn and it matters a lot'\n",
      " 'aquarius and it matters a lot' 'sagittarius and it matters a lot'\n",
      " 'gemini and it matters a lot' 'taurus and it matters a lot'\n",
      " 'virgo and it matters a lot']\n",
      "['sometimes' 'no' nan 'when drinking' 'yes' 'trying to quit']\n",
      "['english' 'english (fluently), spanish (poorly), french (poorly)'\n",
      " 'english, french, c++' ...\n",
      " 'english (fluently), hindi (poorly), french (poorly), tamil (okay), spanish (poorly)'\n",
      " 'english (fluently), french (poorly), japanese (poorly), latin (poorly)'\n",
      " 'english (fluently), french, farsi']\n",
      "['single' 'available' 'seeing someone' 'married' 'unknown']\n"
     ]
    }
   ],
   "source": [
    "#exploration continued\n",
    "# print(df.head())\n",
    "def explorer(data):\n",
    "    ordinals = ['body_type', 'diet', 'drinks', 'drugs', 'education', 'ethnicity', 'job', 'last_online', 'location', 'offspring'\\\n",
    "               ,'orientation', 'pets', 'religion', 'sex', 'sign', 'smokes', 'speaks', 'status']\n",
    "    for column in data[ordinals].select_dtypes(include='object').columns:\n",
    "        print(data[column].unique())\n",
    "explorer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['california' 'colorado' 'new york' 'oregon' 'arizona' 'hawaii' 'montana'\n",
      " 'wisconsin' 'virginia' 'spain' 'nevada' 'illinois' 'vietnam' 'ireland'\n",
      " 'louisiana' 'michigan' 'texas' 'united kingdom' 'massachusetts'\n",
      " 'north carolina' 'idaho' 'mississippi' 'new jersey' 'florida' 'minnesota'\n",
      " 'georgia' 'utah' 'washington' 'west virginia' 'connecticut' 'tennessee'\n",
      " 'rhode island' 'district of columbia' 'british columbia' 'missouri'\n",
      " 'germany' 'pennsylvania' 'netherlands' 'switzerland' 'mexico' 'ohio']\n",
      "[\"doesn't have kids, but might want them\" nan \"doesn't want kids\"\n",
      " \"doesn't have kids, but wants them\" \"doesn't have kids\" 'wants kids'\n",
      " 'has a kid' 'has kids' \"doesn't have kids, and doesn't want any\"\n",
      " \"has kids, but doesn't want more\" \"has a kid, but doesn't want more\"\n",
      " 'has a kid, and wants more' 'has kids, and might want more'\n",
      " 'might want kids' 'has a kid, and might want more'\n",
      " 'has kids, and wants more']\n"
     ]
    }
   ],
   "source": [
    "#many categories have basically nan values better to be nan\n",
    "df.replace(to_replace=-1, value=np.nan)\n",
    "df['body_type'].replace(to_replace='rather not say', value=np.nan)\n",
    "df['diet'].replace(to_replace='strictly other', value='other')\n",
    "df['job'].replace(to_replace='rather not say', value=np.nan)\n",
    "df['status'].replace(to_replace='unknown', value=np.nan)\n",
    "\n",
    "#function to have location be more general\n",
    "def fix_location(data):\n",
    "    new_strings = []\n",
    "    for string in data['location']:\n",
    "        new_strings.append(string.split(', ')[1])\n",
    "    data['location'] = new_strings\n",
    "\n",
    "#function to remove mistaken characters from offspring strings\n",
    "def fix_offspring(data):\n",
    "    new_offspring = []\n",
    "    for string in data['offspring']:\n",
    "        if type(string) is str:\n",
    "            new_offspring.append(\"'\".join(string.split('&rsquo;')))\n",
    "        else:\n",
    "            new_offspring.append(np.nan)\n",
    "    df['offspring'] = new_offspring\n",
    "\n",
    "#same function as for offspring but for sign\n",
    "def fix_sign(data):\n",
    "    new_sign = []\n",
    "    for string in data['sign']:\n",
    "        if type(string) is str:\n",
    "            new_sign.append(\"'\".join(string.split('&rsquo;')))\n",
    "        else:\n",
    "            new_sign.append(np.nan)\n",
    "    df['sign'] = new_sign\n",
    "\n",
    "#only comment in the data has not yet been cleaned\n",
    "# fix_location(df)\n",
    "# fix_offspring(df)\n",
    "# fix_sign(df)\n",
    "print(df['location'].unique())\n",
    "print(df['offspring'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#three approaches to take, nlp from the essay statements to predict, RF for ordinals as dummies, LR for continous \n",
    "#Judging from only binary category is sex this can be used as targets\n",
    "#0 for male, 1 for female\n",
    "# labels = np.where(df['sex'] == 'm', 0, 1)\n",
    "\n",
    "def random_forest(data):\n",
    "    #import useful modules for random forest learning\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, precision_score\n",
    "    from sklearn import tree\n",
    "    \n",
    "    #clean and seperate data in prep for use\n",
    "    good_features = ['body_type', 'diet', 'drinks', 'drugs', 'job', 'location', 'offspring', 'orientation', 'smokes', 'status']\n",
    "    new_data = data.dropna(axis=0, how='any', subset=good_features)\n",
    "    y = np.where(new_data['sex'] == 'm', 0, 1)\n",
    "    x = pd.get_dummies(new_data[good_features], drop_first=True)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    #initiate and train RandomForest over range of max_depth\n",
    "    #self iterated by user over different ranges until best is found\n",
    "    accuracy_test = []\n",
    "    for i in range(14, 40):\n",
    "        rf = RandomForestClassifier(max_depth=i, random_state=42)\n",
    "        rf.fit(x_train, y_train)\n",
    "        y_pred = rf.predict(x_test)\n",
    "        accuracy_test.append(accuracy_score(y_test, y_pred))\n",
    "    best_depth = 14 + np.argmax(accuracy_test)\n",
    "    print(f'best depth: {best_depth}')\n",
    "    \n",
    "    plt.plot(range(14, 40), accuracy_test)\n",
    "    plt.xlabel('Max Depth')\n",
    "    plt.ylabel('Accuracy of Forest')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "    #refit with best depth and determine feature importance\n",
    "    rf = RandomForestClassifier(max_depth = best_depth)\n",
    "    rf.fit(x_train, y_train)\n",
    "    feat_labels = x_train.columns[0:]\n",
    "    importance = rf.feature_importances_\n",
    "    feat_imp = pd.DataFrame(columns=['feature', 'importance'])\n",
    "    feat_imp['feature'] = feat_labels\n",
    "    feat_imp['importance'] = importance\n",
    "    feat_imp.sort_values('importance', axis=0, ascending=False, inplace=True)\n",
    "    print(feat_imp)\n",
    "    \n",
    "    #return predictions of test set can fill with any data that fits the format\n",
    "    return rf.predict(x_test)\n",
    "    \n",
    "# random_forest(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the essay data to try and improve NB accuracy\n",
    "#usage of cleaner does not seem to improve the accuracy of the classifier\n",
    "def cleaner(data):\n",
    "    import re\n",
    "    #remove NaN values\n",
    "    essays = ['essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7', 'essay8', 'essay9']\n",
    "    new_df = data.dropna(axis=0, how='any', subset=essays)\n",
    "        \n",
    "    pattern = r'(\\.|\\,|\\)|\\(|<br />)'\n",
    "    for essay in essays:\n",
    "        new_strings = []\n",
    "        temp = ''\n",
    "        for string in new_df[essay]:\n",
    "            new_strings.append(\"\".join(re.split(pattern, string)).lower())\n",
    "        new_df[essay] = new_strings    \n",
    "    return new_df\n",
    "\n",
    "#explore the use of naive bayes on essay answers to determine gender\n",
    "def nb(data):\n",
    "    #import useful modules for a NB classifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    \n",
    "    #list of essay labels in df\n",
    "    essays = ['essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7', 'essay8', 'essay9']\n",
    "    new_df = data.dropna(axis=0, how='any', subset=essays)\n",
    "    \n",
    "    #assign target labels and essay data for processing\n",
    "    #0 for male, 1 for female\n",
    "    y = np.where(new_df['sex'] == 'm', 0, 1)\n",
    "    \n",
    "    #repeat classification for each essay response\n",
    "    essay_accuracy = []\n",
    "    for essay in essays:\n",
    "        \n",
    "        #initialize counter variable for text\n",
    "        counter = CountVectorizer()\n",
    "        counter.fit(new_df[essay])\n",
    "        #split the data\n",
    "        x = new_df[essay]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        #transform data into usable information\n",
    "        x_train_counts = counter.transform(x_train)\n",
    "        x_test_counts = counter.transform(x_test)\n",
    "    \n",
    "        #init naive bayes classifier\n",
    "        classifier = MultinomialNB()\n",
    "        classifier.fit(x_train_counts, y_train)\n",
    "        essay_accuracy.append(classifier.score(x_test_counts, y_test))\n",
    "    \n",
    "    #prints best accuracy and the essay for which that is\n",
    "    best_accuracy = np.max(essay_accuracy)\n",
    "    index = np.argmax(essay_accuracy)\n",
    "    best_essay = essays[index]\n",
    "    print(f'Best model based on accuracy is {best_essay} with an accuracy of {best_accuracy}')\n",
    "    \n",
    "    #plot accuracy for each essay type\n",
    "    plt.bar(essays, essay_accuracy, tick_label=range(1,10))\n",
    "    plt.xlabel('essay number')\n",
    "    plt.ylabel('accuracy score')\n",
    "    plt.title('Comparison of essay category to accuracy')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "    #find accuracy if using essay pairs as data\n",
    "    i = 0\n",
    "    pair_accuracy = []\n",
    "    pairs = []\n",
    "    while i < len(essays)-1:\n",
    "        n = i+1\n",
    "        #initialize counter variable for text\n",
    "        counter = CountVectorizer()\n",
    "        counter.fit(new_df[essays[i]] + new_df[essays[n]])\n",
    "        x = new_df[essays[i]] + new_df[essays[n]]\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        #transform data into usable information\n",
    "        x_train_counts = counter.transform(x_train)\n",
    "        x_test_counts = counter.transform(x_test)\n",
    "        \n",
    "        #init naive bayes classifier\n",
    "        classifier = MultinomialNB()\n",
    "        classifier.fit(x_train_counts, y_train)\n",
    "        pair_accuracy.append(classifier.score(x_test_counts, y_test))\n",
    "        \n",
    "        pairs.append(f'{i} & {n}')\n",
    "        i = i+2\n",
    "        \n",
    "    #prints best accuracy and the essay for which that is\n",
    "    best_pair = np.max(pair_accuracy)\n",
    "    best_pair_index = np.argmax(pair_accuracy)\n",
    "    pair_max = pairs[best_pair_index]\n",
    "    print(f'Best model based on accuracy is {pair_max} with an accuracy of {best_pair}')\n",
    "    \n",
    "    #plot accuracy for each essay type\n",
    "    plt.bar(pairs, pair_accuracy, tick_label=pairs)\n",
    "    plt.xlabel('essay pairs')\n",
    "    plt.ylabel('accuracy score')\n",
    "    plt.title('Comparison of essay category pairs to accuracy')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "    #find accuracy if used total combined dataset\n",
    "    counter = CountVectorizer()\n",
    "    counter.fit(new_df[essays])\n",
    "    x = new_df['essay1'] + new_df['essay2'] + new_df['essay3'] + new_df['essay4'] + new_df['essay5'] + new_df['essay6'] + new_df['essay7'] + new_df['essay8'] + new_df['essay9']\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    #transform data into usable information\n",
    "    x_train_counts = counter.transform(x_train)\n",
    "    x_test_counts = counter.transform(x_test)\n",
    "\n",
    "    #init naive bayes classifier\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(x_train_counts, y_train)\n",
    "    model_accuracy = classifier.score(x_test_counts, y_test)\n",
    "    print(f'For combined essays model accuracy is {model_accuracy}')\n",
    "# new_data = cleaner(df)\n",
    "# nb(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method 3 normalize and use LR on contiinous data then whole data\n",
    "def normalise(data):\n",
    "    #import useful mods\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    conts = ['age', 'height', 'income']\n",
    "    new_data = data.dropna(axis=0, how='any', subset=conts)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    x = scaler.fit_transform(new_data[conts])\n",
    "    y = new_data['sex']\n",
    "    return x, y\n",
    "\n",
    "x, y = normalise(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has accuracy 0.8335\n"
     ]
    }
   ],
   "source": [
    "#perform the LR on the prepared data\n",
    "def linear_mod(x, y):\n",
    "    #import useful mods\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    #seperate data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(x_train, y_train)\n",
    "    score = round(lr.score(x_test, y_test), 4)\n",
    "    print(f'Model has accuracy {score}')\n",
    "\n",
    "linear_mod(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is 0.8800841514726507\n",
      "\n",
      "Confusion Matrix for results\n",
      "[[568 103]\n",
      " [ 68 687]]\n"
     ]
    }
   ],
   "source": [
    "#attempt to feature engineer the categorical dataset for a LR model\n",
    "def total_lr(data):\n",
    "    #import useful mods\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    #import column lists\n",
    "    to_drop = ['essay0', 'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7', 'essay8', 'essay9'\\\n",
    "              ,'last_online']\n",
    "    ordinals = ['body_type', 'diet', 'drinks', 'drugs', 'education', 'ethnicity', 'job', 'location', 'offspring'\\\n",
    "               ,'orientation', 'pets', 'religion', 'sign', 'smokes', 'speaks', 'status']\n",
    "    conts = ['age', 'height', 'income']\n",
    "    \n",
    "    #clean the data slightly\n",
    "    new_data = data.drop(to_drop, axis=1)\n",
    "    new_data.dropna(axis=0, how='any', subset=ordinals+conts, inplace=True)\n",
    "    \n",
    "    #OHE the categoric variables\n",
    "    for column in ordinals:\n",
    "        holder = pd.get_dummies(new_data[column], prefix=column)\n",
    "        new_data = pd.concat([new_data, holder], axis=1)\n",
    "        new_data = new_data.drop(column, axis=1)\n",
    "    \n",
    "    #scale the continuous variables\n",
    "    scaler = StandardScaler()\n",
    "    new_data[conts] = scaler.fit_transform(new_data[conts])\n",
    "    \n",
    "    #seperate data\n",
    "    y = new_data['sex']\n",
    "    x = new_data.drop('sex', axis=1)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    #perform LR model\n",
    "    lr = LogisticRegression(max_iter=200)\n",
    "    lr.fit(x_train, y_train)\n",
    "    score = lr.score(x_test, y_test)\n",
    "    print(f'Model accuracy is {score}')\n",
    "    \n",
    "    #visualise\n",
    "    y_pred = lr.predict(x_test)\n",
    "    print('\\nConfusion Matrix for results')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "total_lr(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
